{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6dcd151-b310-4e0a-98b6-622608e69d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import csv\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d948bd4c-4dc4-4cef-91a2-a22cf4f4a472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication Successfull\n",
      "2021-09-08 2021-09-09\n"
     ]
    }
   ],
   "source": [
    "API_KEY = ''\n",
    "SECRET_KEY = ''\n",
    "BEARER_TOKEN = ''\n",
    "ACCESS_TOKEN = ''\n",
    "SECRET_TOKEN = ''\n",
    "\n",
    "try:\n",
    "    auth = tw.OAuthHandler(API_KEY, SECRET_KEY)\n",
    "    auth.set_access_token(ACCESS_TOKEN, SECRET_TOKEN)\n",
    "    api = tw.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    print(\"Authentication Successfull\")\n",
    "except:\n",
    "    print(\"Error: Authentication Failed\")\n",
    "    \n",
    "today = datetime.date.today()\n",
    "yesterday= today - datetime.timedelta(days=1)\n",
    "end = yesterday\n",
    "start = end - datetime.timedelta(days=1)\n",
    "print(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "868c048d-8817-4fbb-bf15-07af0cfce4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(txt):\n",
    "    #return \" \".join(re.sub(\"([^0-9A-Za-zğüşöçıİĞÜŞÖÇ \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d4aa3ce-45e3-441b-bec0-e371fe02e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words1 = \"asıolmayacağım -filter:retweets\"\n",
    "tweets_list1 = tw.Cursor(api.search, q=search_words1,tweet_mode='extended', lang='tr', since = start, until = end).items(10000)\n",
    "#tweets_no_urls = [remove_url(tweet._json[\"full_text\"]) for tweet in tweets_list]\n",
    "\n",
    "search_words2 = \"AsıVePcrDurdurulsun -filter:retweets\"\n",
    "tweets_list2 = tw.Cursor(api.search, q=search_words2,tweet_mode='extended', lang='tr', since = start, until = end).items(10000)\n",
    "\n",
    "search_words3 = \"AşıMaşıOlmıycam -filter:retweets\"\n",
    "tweets_list3 = tw.Cursor(api.search, q=search_words3,tweet_mode='extended', lang='tr', since = start, until = end).items(10000)\n",
    "\n",
    "search_words4 = \"BioNTechYanEtki -filter:retweets\"\n",
    "tweets_list4 = tw.Cursor(api.search, q=search_words4,tweet_mode='extended', lang='tr', since = start, until = end).items(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbf74594-d502-4c92-b897-a39a35c50b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dataset = pd.DataFrame(columns=['Tweet Id', 'Tweet Date', 'Follower Count', 'Account Verified', 'Favorite Count', 'Retweets', 'Tweet Text'])\n",
    "outfilename = \"data/erewr\" + str(start) + \"_\" + str(end)+\".xlsx\"\n",
    "tweet_dataset.to_excel(outfilename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a1f59d1-cfdf-406b-bcd4-bb02cc6dd7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 760\n",
      "Rate limit reached. Sleeping for: 758\n"
     ]
    }
   ],
   "source": [
    "tweet_dataset = pd.DataFrame(columns=['Tweet Id', 'Tweet Date', 'Follower Count', 'Account Verified', 'Favorite Count', 'Retweets', 'Tweet Text'])\n",
    "for tweet in tweets_list1:\n",
    "    #print(tweet._json[\"full_text\"])\n",
    "    appending_dataframe = pd.DataFrame([[tweet.id, tweet.created_at, tweet.user.followers_count, tweet.user.verified, tweet.favorite_count, tweet.retweet_count, remove_url(tweet._json[\"full_text\"])]], columns=['Tweet Id', 'Tweet Date', 'Follower Count', 'Account Verified', 'Favorite Count', 'Retweets', 'Tweet Text'])\n",
    "    tweet_dataset = tweet_dataset.append(appending_dataframe)\n",
    "\n",
    "for tweet in tweets_list2:\n",
    "    #print(tweet._json[\"full_text\"])\n",
    "    appending_dataframe = pd.DataFrame([[tweet.id, tweet.created_at, tweet.user.followers_count, tweet.user.verified, tweet.favorite_count, tweet.retweet_count, remove_url(tweet._json[\"full_text\"])]], columns=['Tweet Id', 'Tweet Date', 'Follower Count', 'Account Verified', 'Favorite Count', 'Retweets', 'Tweet Text'])\n",
    "    tweet_dataset = tweet_dataset.append(appending_dataframe)\n",
    "    \n",
    "for tweet in tweets_list3:\n",
    "    #print(tweet._json[\"full_text\"])\n",
    "    appending_dataframe = pd.DataFrame([[tweet.id, tweet.created_at, tweet.user.followers_count, tweet.user.verified, tweet.favorite_count, tweet.retweet_count, remove_url(tweet._json[\"full_text\"])]], columns=['Tweet Id', 'Tweet Date', 'Follower Count', 'Account Verified', 'Favorite Count', 'Retweets', 'Tweet Text'])\n",
    "    tweet_dataset = tweet_dataset.append(appending_dataframe)\n",
    "\n",
    "for tweet in tweets_list4:\n",
    "    #print(tweet._json[\"full_text\"])\n",
    "    appending_dataframe = pd.DataFrame([[tweet.id, tweet.created_at, tweet.user.followers_count, tweet.user.verified, tweet.favorite_count, tweet.retweet_count, remove_url(tweet._json[\"full_text\"])]], columns=['Tweet Id', 'Tweet Date', 'Follower Count', 'Account Verified', 'Favorite Count', 'Retweets', 'Tweet Text'])\n",
    "    tweet_dataset = tweet_dataset.append(appending_dataframe)\n",
    "    \n",
    "outfilename = \"data/against_full_\" + str(start) + \"_\" + str(end)+\".xlsx\"\n",
    "tweet_dataset.to_excel(outfilename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d6904bd-fd68-475c-98d7-79daa6138975",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words1 = \"covid-19 -filter:retweets\"\n",
    "tweets_list1 = tw.Cursor(api.search, q=search_words1,tweet_mode='extended', lang='tr', since = start, until = end).items(10000)\n",
    "#tweets_no_urls = [remove_url(tweet._json[\"full_text\"]) for tweet in tweets_list]\n",
    "\n",
    "search_words2 = \"biontech -filter:retweets\"\n",
    "tweets_list2 = tw.Cursor(api.search, q=search_words2,tweet_mode='extended', lang='tr', since = start, until = end).items(10000)\n",
    "\n",
    "search_words3 = \"mrna -filter:retweets\"\n",
    "tweets_list3 = tw.Cursor(api.search, q=search_words3,tweet_mode='extended', lang='tr', since = start, until = end).items(10000)\n",
    "\n",
    "search_words4 = \"AşıŞart -filter:retweets\"\n",
    "tweets_list4 = tw.Cursor(api.search, q=search_words4,tweet_mode='extended', lang='tr', since = start, until = end).items(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7ec9797-bb9f-4d6f-ada4-1cdb638682a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 751\n",
      "Rate limit reached. Sleeping for: 746\n"
     ]
    }
   ],
   "source": [
    "tweet_dataset2 = pd.DataFrame(columns=['Tweet Id', 'Tweet Date', 'Follower Count', 'Account Verified', 'Favorite Count', 'Retweets', 'Tweet Text'])\n",
    "for tweet in tweets_list1:\n",
    "    #print(tweet._json[\"full_text\"])\n",
    "    appending_dataframe = pd.DataFrame([[tweet.id, tweet.created_at, tweet.user.followers_count, tweet.user.verified, tweet.favorite_count, tweet.retweet_count, remove_url(tweet._json[\"full_text\"])]], columns=['Tweet Id', 'Tweet Date', 'Follower Count', 'Account Verified', 'Favorite Count', 'Retweets', 'Tweet Text'])\n",
    "    tweet_dataset2 = tweet_dataset2.append(appending_dataframe)\n",
    "\n",
    "for tweet in tweets_list2:\n",
    "    #print(tweet._json[\"full_text\"])\n",
    "    appending_dataframe = pd.DataFrame([[tweet.id, tweet.created_at, tweet.user.followers_count, tweet.user.verified, tweet.favorite_count, tweet.retweet_count, remove_url(tweet._json[\"full_text\"])]], columns=['Tweet Id', 'Tweet Date', 'Follower Count', 'Account Verified', 'Favorite Count', 'Retweets', 'Tweet Text'])\n",
    "    tweet_dataset2 = tweet_dataset2.append(appending_dataframe)\n",
    "    \n",
    "for tweet in tweets_list3:\n",
    "    #print(tweet._json[\"full_text\"])\n",
    "    appending_dataframe = pd.DataFrame([[tweet.id, tweet.created_at, tweet.user.followers_count, tweet.user.verified, tweet.favorite_count, tweet.retweet_count, remove_url(tweet._json[\"full_text\"])]], columns=['Tweet Id', 'Tweet Date', 'Follower Count', 'Account Verified', 'Favorite Count', 'Retweets', 'Tweet Text'])\n",
    "    tweet_dataset2 = tweet_dataset2.append(appending_dataframe)\n",
    "\n",
    "for tweet in tweets_list4:\n",
    "    #print(tweet._json[\"full_text\"])\n",
    "    appending_dataframe = pd.DataFrame([[tweet.id, tweet.created_at, tweet.user.followers_count, tweet.user.verified, tweet.favorite_count, tweet.retweet_count, remove_url(tweet._json[\"full_text\"])]], columns=['Tweet Id', 'Tweet Date', 'Follower Count', 'Account Verified', 'Favorite Count', 'Retweets', 'Tweet Text'])\n",
    "    tweet_dataset2 = tweet_dataset2.append(appending_dataframe)\n",
    "    \n",
    "outfilename = \"data/belirsiz_full_\" + str(start) + \"_\" + str(end)+\".xlsx\"\n",
    "tweet_dataset2.to_excel(outfilename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9938d6-1719-4a31-9be8-b3842ebfa682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
